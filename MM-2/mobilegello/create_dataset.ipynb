{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd4eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #   Utility Functions\n",
    "# # \n",
    "# # Function to extract the minute and second parts of the timestamp from the filename\n",
    "\n",
    "# from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def get_timestamp_from_filename(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split('_')\n",
    "    hr = int(parts[-5])\n",
    "    minute = int(parts[-4])\n",
    "    second = int(parts[-3].split('.')[0])  # Removing the \".csv\" part\n",
    "    return (hr, minute, second)\n",
    "\n",
    "# def eul_2_quat(rpy):\n",
    "#     rotation = R.from_rotvec(rpy)\n",
    "#     return rotation.as_quat()\n",
    "\n",
    "# def quat_2_eul(quat):\n",
    "#     rotation = R.from_quat(quat)\n",
    "#     return rotation.as_euler('xyz', degrees=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5616466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV File: lift_data/data/2024_10_31_17_11_15_ep_0.csv\n",
      "length of demo: 47\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_17_41_ep_1.csv\n",
      "length of demo: 28\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_22_41_ep_3.csv\n",
      "length of demo: 35\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_23_32_ep_4.csv\n",
      "length of demo: 38\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_24_13_ep_2.csv\n",
      "length of demo: 43\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_24_50_ep_5.csv\n",
      "length of demo: 37\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_26_11_ep_6.csv\n",
      "length of demo: 43\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_26_50_ep_7.csv\n",
      "length of demo: 51\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_27_45_ep_8.csv\n",
      "length of demo: 41\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_28_25_ep_9.csv\n",
      "length of demo: 42\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_29_24_ep_10.csv\n",
      "length of demo: 31\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_31_21_ep_11.csv\n",
      "length of demo: 34\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_33_46_ep_12.csv\n",
      "length of demo: 43\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_34_26_ep_13.csv\n",
      "length of demo: 38\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_35_10_ep_14.csv\n",
      "length of demo: 41\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_35_48_ep_15.csv\n",
      "length of demo: 42\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_36_33_ep_16.csv\n",
      "length of demo: 35\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_37_06_ep_17.csv\n",
      "length of demo: 34\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_38_01_ep_18.csv\n",
      "length of demo: 36\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_41_07_ep_19.csv\n",
      "length of demo: 42\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_41_51_ep_20.csv\n",
      "length of demo: 40\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_44_26_ep_21.csv\n",
      "length of demo: 37\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_45_23_ep_22.csv\n",
      "length of demo: 32\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_45_54_ep_23.csv\n",
      "length of demo: 34\n",
      "Reading CSV File: lift_data/data/2024_10_31_17_46_27_ep_24.csv\n",
      "length of demo: 40\n",
      "----------------------------\n",
      "Total number of demos: 25\n",
      "----------------------------\n",
      "data: dict_keys(['demo_0', 'demo_1', 'demo_2', 'demo_3', 'demo_4', 'demo_5', 'demo_6', 'demo_7', 'demo_8', 'demo_9', 'demo_10', 'demo_11', 'demo_12', 'demo_13', 'demo_14', 'demo_15', 'demo_16', 'demo_17', 'demo_18', 'demo_19', 'demo_20', 'demo_21', 'demo_22', 'demo_23', 'demo_24'])\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV Files - create a dictionary with the data\n",
    "\n",
    "csv_folder = \"lift_data/data\"\n",
    "data = {}\n",
    "\n",
    "csv_files = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "sorted_file_paths = sorted(csv_files, key=get_timestamp_from_filename)\n",
    "# Sort the CSV files based on name\n",
    "# print(sorted_file_paths)\n",
    "for n, csv_file in enumerate(sorted_file_paths):\n",
    "\n",
    "\n",
    "    print(f\"Reading CSV File: {csv_file}\")\n",
    "    timestamps = pd.read_csv(csv_file, usecols=[0], engine=\"python\")\n",
    "    joint_angles = pd.read_csv(csv_file, usecols=range(1, 7), engine=\"python\").astype(np.float64)\n",
    "\n",
    "    gripper_pos = np.zeros((len(joint_angles), 1))\n",
    "    # gripper_pos = pd.read_csv(csv_file, usecols=[7], engine=\"python\").astype(np.float64)\n",
    "    gripper_pos = np.array(gripper_pos)\n",
    "    \n",
    "\n",
    "    ee_pos_xyz = pd.read_csv(csv_file, usecols=range(7, 10), engine=\"python\").astype(np.float64)\n",
    "    # convert into meters\n",
    "    ee_pos_xyz = ee_pos_xyz/1000\n",
    "    \n",
    "    # ee_pos_xyz = ee_pos_xyz\n",
    "    # ee_pos_rpy = pd.read_csv(csv_file, usecols=range(11, 14), engine=\"python\").astype(np.float64)\n",
    "\n",
    "    # Create a nested dictionary for each demonstration\n",
    "    data[f'demo_{n}'] = {\n",
    "        'timestamps': np.array(timestamps)[::2],\n",
    "        'joint_angles': np.array(joint_angles)[::2],\n",
    "        'gripper_pos': np.array(gripper_pos)[::2],\n",
    "        'ee_pos_xyz': np.array(ee_pos_xyz)[::2],\n",
    "        # 'timestamps': np.array(timestamps),\n",
    "        # 'joint_angles': np.array(joint_angles),\n",
    "        # 'gripper_pos': np.array(gripper_pos),\n",
    "        # 'ee_pos_xyz': np.array(ee_pos_xyz),\n",
    "        # convert rpy to quaternion\n",
    "        # 'ee_pos_quat': np.array(eul_2_quat(np.array(ee_pos_rpy)[::2])),\n",
    "    }\n",
    "    print(f\"length of demo: {len(data[f'demo_{n}']['gripper_pos'])}\")\n",
    "\n",
    "    # if n == 9:      # Only 10 trajectory\n",
    "    #     break\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(f\"Total number of demos: {len(data)}\")\n",
    "print(\"----------------------------\")\n",
    "print(f\"data: {data.keys()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b88caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Image Folder: lift_data/images/2024_10_31_17_26_44_ep_7\n",
      "Number of images: 51\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_24_06_ep_2\n",
      "Number of images: 44\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_24_45_ep_5\n",
      "Number of images: 38\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_41_40_ep_20\n",
      "Number of images: 40\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_23_22_ep_4\n",
      "Number of images: 40\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_33_36_ep_12\n",
      "Number of images: 40\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_36_26_ep_16\n",
      "Number of images: 37\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_46_18_ep_24\n",
      "Number of images: 32\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_22_35_ep_3\n",
      "Number of images: 32\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_35_02_ep_14\n",
      "Number of images: 40\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_26_05_ep_6\n",
      "Number of images: 38\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_35_40_ep_15\n",
      "Number of images: 44\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_40_57_ep_19\n",
      "Number of images: 43\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_11_11_ep_0\n",
      "Number of images: 41\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_37_00_ep_17\n",
      "Number of images: 34\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_27_33_ep_8\n",
      "Number of images: 43\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_28_56_ep_10\n",
      "Number of images: 30\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_31_15_ep_11\n",
      "Number of images: 35\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_34_17_ep_13\n",
      "Number of images: 38\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_45_17_ep_22\n",
      "Number of images: 31\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_17_37_ep_1\n",
      "Number of images: 25\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_45_50_ep_23\n",
      "Number of images: 33\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_28_19_ep_9\n",
      "Number of images: 41\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_37_53_ep_18\n",
      "Number of images: 37\n",
      "Reading Image Folder: lift_data/images/2024_10_31_17_43_51_ep_21\n",
      "Number of images: 36\n",
      "----------------------------------------\n",
      "Total number of images folders: 25\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read Images [cam0 - Env Camera] folder - Create a dictionary with the images\n",
    "\n",
    "import cv2\n",
    "\n",
    "images = \"\"\n",
    "\n",
    "image_folders = glob.glob(os.path.join(\"lift_data/images\", '*'))\n",
    "# sorted_image_folders = sorted(image_folders, key=get_timestamp_from_filename)\n",
    "sorted_image_folders = image_folders\n",
    "\n",
    "# Sort the CSV files based on name\n",
    "for n, image_folder in enumerate(sorted_image_folders):\n",
    "    print(f\"Reading Image Folder: {image_folder}\")\n",
    "    image_files = glob.glob(os.path.join(image_folder, '*.png'))\n",
    "    \n",
    "    # get number of images\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    # print(sorted_image_files)\n",
    "    # get all image files as numpy array\n",
    "    cv_images=[]\n",
    "    orig_images = []\n",
    "    for i in range(0, num_images, 2):\n",
    "        cv_image = cv2.imread(f\"{image_folder}/frame_{i}.png\")\n",
    "        \n",
    "        orig_images.append(cv_image)\n",
    "        cv_image = cv2.resize(cv_image, (96, 96))\n",
    "        # Transpose the image to (3, 320, 320)\n",
    "        cv_image = np.transpose(cv_image, (2, 0, 1))\n",
    "        cv_images.append(cv_image)\n",
    "\n",
    "    print(f\"Number of images: {len(cv_images)}\")\n",
    "    # Create a nested dictionary for each demonstration\n",
    "    data[f'demo_{n}']['cam0'] = np.array(cv_images)\n",
    "    data[f\"demo_{n}\"]['cam0_orig'] = np.array(orig_images)\n",
    "\n",
    "    # if n == 9:      # Only 10 datasets\n",
    "    #     break\n",
    "\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"Total number of images folders: {len(sorted_image_folders)}\")\n",
    "print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c860c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read Images [cam1 - In-Hand Camera] folder - Create a dictionary with the images\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# images = \"\"\n",
    "\n",
    "# image_folders = glob.glob(os.path.join(\"data/ButtonPush/cam1\", '*'))\n",
    "# sorted_image_folders = sorted(image_folders, key=get_timestamp_from_filename)\n",
    "# # Sort the CSV files based on name\n",
    "# for n, image_folder in enumerate(sorted_image_folders):\n",
    "#     print(f\"Reading Image Folder: {image_folder}\")\n",
    "#     image_files = glob.glob(os.path.join(image_folder, '*.png'))\n",
    "    \n",
    "#     # Sort the image files based on name\n",
    "#     sorted_image_files = sorted(image_files, key=get_timestamp_from_filename)\n",
    "\n",
    "#     # print(sorted_image_files)\n",
    "#     # get all image files as numpy array\n",
    "#     cv_images=[]\n",
    "#     orig_images = []\n",
    "#     for i in range(0, len(sorted_image_files), 1):\n",
    "#         cv_image = cv2.imread(sorted_image_files[i])\n",
    "#         orig_images.append(cv_image)\n",
    "#         # cv_image = cv2.rotate(cv_image, cv2.ROTATE_90_CLOCKWISE)       # Temporary Edit\n",
    "#         cv_image = cv2.resize(cv_image, (96, 96))\n",
    "#         # Transpose the image to (3, 320, 320)\n",
    "#         cv_image = np.transpose(cv_image, (2, 0, 1))\n",
    "#         cv_images.append(cv_image)\n",
    "\n",
    "#     print(f\"Number of images: {len(cv_images)}\")\n",
    "#     # Create a nested dictionary for each demonstration\n",
    "#     data[f'demo_{n}']['cam1'] = np.array(cv_images)\n",
    "#     data[f'demo_{n}']['cam1_orig'] = np.array(orig_images)\n",
    "\n",
    "\n",
    "#     # if n == 9:      # Only 10 datasets\n",
    "#         # break\n",
    "\n",
    "\n",
    "\n",
    "# print(\"----------------------------------------\")\n",
    "# print(f\"Total number of images folders: {len(sorted_image_folders)}\")\n",
    "# print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8652ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read Images [cam1 - In-Hand Camera] folder - Create a dictionary with the images\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# images = \"\"\n",
    "\n",
    "# image_folders = glob.glob(os.path.join(\"data/ButtonPush/cam2\", '*'))\n",
    "# sorted_image_folders = sorted(image_folders, key=get_timestamp_from_filename)\n",
    "# # Sort the CSV files based on name\n",
    "# for n, image_folder in enumerate(sorted_image_folders):\n",
    "#     print(f\"Reading Image Folder: {image_folder}\")\n",
    "#     image_files = glob.glob(os.path.join(image_folder, '*.png'))\n",
    "    \n",
    "#     # Sort the image files based on name\n",
    "#     sorted_image_files = sorted(image_files, key=get_timestamp_from_filename)\n",
    "\n",
    "#     # print(sorted_image_files)\n",
    "#     # get all image files as numpy array\n",
    "#     cv_images=[]\n",
    "#     orig_images = []\n",
    "#     for i in range(0, len(sorted_image_files), 1):\n",
    "#         cv_image = cv2.imread(sorted_image_files[i])\n",
    "#         orig_images.append(cv_image)\n",
    "#         # cv_image = cv2.rotate(cv_image, cv2.ROTATE_90_CLOCKWISE)       # Temporary Edit\n",
    "#         cv_image = cv2.resize(cv_image, (96, 96))\n",
    "#         # Transpose the image to (3, 320, 320)\n",
    "#         cv_image = np.transpose(cv_image, (2, 0, 1))\n",
    "#         cv_images.append(cv_image)\n",
    "\n",
    "#     print(f\"Number of images: {len(cv_images)}\")\n",
    "#     # Create a nested dictionary for each demonstration\n",
    "#     data[f'demo_{n}']['cam2'] = np.array(cv_images)\n",
    "#     data[f'demo_{n}']['cam2_orig'] = np.array(orig_images)\n",
    "\n",
    "\n",
    "#     # if n == 9:      # Only 10 datasets\n",
    "#         # break\n",
    "\n",
    "\n",
    "\n",
    "# print(\"----------------------------------------\")\n",
    "# print(f\"Total number of images folders: {len(sorted_image_folders)}\")\n",
    "# print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1ade5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data demo_0: 47 timestamps, cam0: 51\n",
      "i\n",
      "Data demo_1: 28 timestamps, cam0: 44\n",
      "i\n",
      "Data demo_2: 35 timestamps, cam0: 38\n",
      "i\n",
      "Data demo_3: 38 timestamps, cam0: 40\n",
      "i\n",
      "Data demo_4: 43 timestamps, cam0: 40\n",
      "i\n",
      "Data demo_5: 37 timestamps, cam0: 40\n",
      "i\n",
      "Data demo_6: 43 timestamps, cam0: 37\n",
      "i\n",
      "Data demo_7: 51 timestamps, cam0: 32\n",
      "i\n",
      "Data demo_8: 41 timestamps, cam0: 32\n",
      "i\n",
      "Data demo_9: 42 timestamps, cam0: 40\n",
      "i\n",
      "Data demo_10: 31 timestamps, cam0: 38\n",
      "i\n",
      "Data demo_11: 34 timestamps, cam0: 44\n",
      "i\n",
      "Data demo_12: 43 timestamps, cam0: 43\n",
      "i\n",
      "Data demo_13: 38 timestamps, cam0: 41\n",
      "i\n",
      "Data demo_14: 41 timestamps, cam0: 34\n",
      "i\n",
      "Data demo_15: 42 timestamps, cam0: 43\n",
      "i\n",
      "Data demo_16: 35 timestamps, cam0: 30\n",
      "i\n",
      "Data demo_17: 34 timestamps, cam0: 35\n",
      "i\n",
      "Data demo_18: 36 timestamps, cam0: 38\n",
      "i\n",
      "Data demo_19: 42 timestamps, cam0: 31\n",
      "i\n",
      "Data demo_20: 40 timestamps, cam0: 25\n",
      "i\n",
      "Data demo_21: 37 timestamps, cam0: 33\n",
      "i\n",
      "Data demo_22: 32 timestamps, cam0: 41\n",
      "i\n",
      "Data demo_23: 34 timestamps, cam0: 37\n",
      "i\n",
      "Data demo_24: 40 timestamps, cam0: 36\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# Compare the length of csv files to the number of images in the folder\n",
    "\n",
    "for demo in data:\n",
    "    # print(f\"Data {demo}: {len(data[demo]['ee_pos_xyz'])} timestamps, cam0: {len(data[demo]['cam0'])}, cam1: {len(data[demo]['cam1'])}, cam2: {len(data[demo]['cam2'])}\",)\n",
    "    print(f\"Data {demo}: {len(data[demo]['ee_pos_xyz'])} timestamps, cam0: {len(data[demo]['cam0'])}\")\n",
    "    print(\"i\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811cb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "105f9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After matching: Data demo_0: 47 timestamps, cam0: 47\n",
      "After matching: Data demo_1: 28 timestamps, cam0: 28\n",
      "After matching: Data demo_2: 35 timestamps, cam0: 35\n",
      "After matching: Data demo_3: 38 timestamps, cam0: 38\n",
      "After matching: Data demo_4: 40 timestamps, cam0: 40\n",
      "After matching: Data demo_5: 37 timestamps, cam0: 37\n",
      "After matching: Data demo_6: 37 timestamps, cam0: 37\n",
      "After matching: Data demo_7: 32 timestamps, cam0: 32\n",
      "After matching: Data demo_8: 32 timestamps, cam0: 32\n",
      "After matching: Data demo_9: 40 timestamps, cam0: 40\n",
      "After matching: Data demo_10: 31 timestamps, cam0: 31\n",
      "After matching: Data demo_11: 34 timestamps, cam0: 34\n",
      "After matching: Data demo_12: 43 timestamps, cam0: 43\n",
      "After matching: Data demo_13: 38 timestamps, cam0: 38\n",
      "After matching: Data demo_14: 34 timestamps, cam0: 34\n",
      "After matching: Data demo_15: 42 timestamps, cam0: 42\n",
      "After matching: Data demo_16: 30 timestamps, cam0: 30\n",
      "After matching: Data demo_17: 34 timestamps, cam0: 34\n",
      "After matching: Data demo_18: 36 timestamps, cam0: 36\n",
      "After matching: Data demo_19: 31 timestamps, cam0: 31\n",
      "After matching: Data demo_20: 25 timestamps, cam0: 25\n",
      "After matching: Data demo_21: 33 timestamps, cam0: 33\n",
      "After matching: Data demo_22: 32 timestamps, cam0: 32\n",
      "After matching: Data demo_23: 34 timestamps, cam0: 34\n",
      "After matching: Data demo_24: 36 timestamps, cam0: 36\n"
     ]
    }
   ],
   "source": [
    "# Match the lengths of timestamps and images\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def match_lengths(data):\n",
    "    for demo in data:\n",
    "        num_timestamps = len(data[demo]['ee_pos_xyz'])\n",
    "        cam0 = len(data[demo]['cam0'])\n",
    "        # cam1 = len(data[demo]['cam1'])\n",
    "        # cam2 = len(data[demo]['cam2'])\n",
    "\n",
    "        # Find the minimum length among timestamps and cameras\n",
    "        min_len = min(num_timestamps, cam0)\n",
    "\n",
    "        # Truncate data to match the smallest length\n",
    "        data[demo]['ee_pos_xyz'] = data[demo]['ee_pos_xyz'][:min_len]\n",
    "        data[demo]['cam0'] = data[demo]['cam0'][:min_len]\n",
    "        # data[demo]['cam1'] = data[demo]['cam1'][:min_len]\n",
    "        # data[demo]['cam2'] = data[demo]['cam2'][:min_len]\n",
    "        data[demo]['gripper_pos'] = data[demo]['gripper_pos'][:min_len]\n",
    "        data[demo]['cam0_orig'] = data[demo]['cam0_orig'][:min_len]\n",
    "        # data[demo]['cam1_orig'] = data[demo]['cam1_orig'][:min_len]\n",
    "        # data[demo]['cam2_orig'] = data[demo]['cam2_orig'][:min_len]\n",
    "\n",
    "        # Debug prints to verify the matching\n",
    "        # print(f\"After matching: Data {demo}: {len(data[demo]['ee_pos_xyz'])} timestamps, cam0: {len(data[demo]['cam0'])}, cam1: {len(data[demo]['cam1'])}, cam2: {len(data[demo]['cam2'])}\")\n",
    "        print(f\"After matching: Data {demo}: {len(data[demo]['ee_pos_xyz'])} timestamps, cam0: {len(data[demo]['cam0'])}\")\n",
    "\n",
    "# Example usage\n",
    "match_lengths(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8042d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Preprocess the Gripper Position\n",
    "\n",
    "# # indicies = np.array([44, 44, 43, 36, 44, 40, 34, 42, 48, 40])\n",
    "# # print(indicies)\n",
    "\n",
    "# # for n, i in enumerate(data.keys()):\n",
    "# #     gripper_pos = data[i]['gripper_pos']\n",
    "\n",
    "# #     # -------------- Change gripper pos at corresponding index --------------------\n",
    "# #     gripper_pos[:indicies[n]] = 0.0\n",
    "# #     gripper_pos[indicies[n]:] = 0.5\n",
    "\n",
    "# #     data[i]['gripper_pos'] = gripper_pos\n",
    "\n",
    "# # print(\"Gripper Poses Updated\")\n",
    "\n",
    "\n",
    "# ## Approximate gripper position - 0.6\n",
    "\n",
    "# for demo in data.keys():\n",
    "#     gripper_pos = data[demo]['gripper_pos']\n",
    "#     gripper_pos_new = np.where(gripper_pos > 0.1, 0.6, 0.0)\n",
    "#     data[demo]['gripper_pos'] = gripper_pos_new \n",
    "    \n",
    "# print(\"--------------------------------------\")\n",
    "# print(\"Gripper Pos set to 0.0 or 0.6\")\n",
    "# print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6407fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo: demo_0, Gripper Pos: (47, 1)\n",
      "Actions Added: Dimension: (46, 4)\n",
      "Demo: demo_1, Gripper Pos: (28, 1)\n",
      "Actions Added: Dimension: (27, 4)\n",
      "Demo: demo_2, Gripper Pos: (35, 1)\n",
      "Actions Added: Dimension: (34, 4)\n",
      "Demo: demo_3, Gripper Pos: (38, 1)\n",
      "Actions Added: Dimension: (37, 4)\n",
      "Demo: demo_4, Gripper Pos: (40, 1)\n",
      "Actions Added: Dimension: (39, 4)\n",
      "Demo: demo_5, Gripper Pos: (37, 1)\n",
      "Actions Added: Dimension: (36, 4)\n",
      "Demo: demo_6, Gripper Pos: (37, 1)\n",
      "Actions Added: Dimension: (36, 4)\n",
      "Demo: demo_7, Gripper Pos: (32, 1)\n",
      "Actions Added: Dimension: (31, 4)\n",
      "Demo: demo_8, Gripper Pos: (32, 1)\n",
      "Actions Added: Dimension: (31, 4)\n",
      "Demo: demo_9, Gripper Pos: (40, 1)\n",
      "Actions Added: Dimension: (39, 4)\n",
      "Demo: demo_10, Gripper Pos: (31, 1)\n",
      "Actions Added: Dimension: (30, 4)\n",
      "Demo: demo_11, Gripper Pos: (34, 1)\n",
      "Actions Added: Dimension: (33, 4)\n",
      "Demo: demo_12, Gripper Pos: (43, 1)\n",
      "Actions Added: Dimension: (42, 4)\n",
      "Demo: demo_13, Gripper Pos: (38, 1)\n",
      "Actions Added: Dimension: (37, 4)\n",
      "Demo: demo_14, Gripper Pos: (34, 1)\n",
      "Actions Added: Dimension: (33, 4)\n",
      "Demo: demo_15, Gripper Pos: (42, 1)\n",
      "Actions Added: Dimension: (41, 4)\n",
      "Demo: demo_16, Gripper Pos: (30, 1)\n",
      "Actions Added: Dimension: (29, 4)\n",
      "Demo: demo_17, Gripper Pos: (34, 1)\n",
      "Actions Added: Dimension: (33, 4)\n",
      "Demo: demo_18, Gripper Pos: (36, 1)\n",
      "Actions Added: Dimension: (35, 4)\n",
      "Demo: demo_19, Gripper Pos: (31, 1)\n",
      "Actions Added: Dimension: (30, 4)\n",
      "Demo: demo_20, Gripper Pos: (25, 1)\n",
      "Actions Added: Dimension: (24, 4)\n",
      "Demo: demo_21, Gripper Pos: (33, 1)\n",
      "Actions Added: Dimension: (32, 4)\n",
      "Demo: demo_22, Gripper Pos: (32, 1)\n",
      "Actions Added: Dimension: (31, 4)\n",
      "Demo: demo_23, Gripper Pos: (34, 1)\n",
      "Actions Added: Dimension: (33, 4)\n",
      "Demo: demo_24, Gripper Pos: (36, 1)\n",
      "Actions Added: Dimension: (35, 4)\n"
     ]
    }
   ],
   "source": [
    "# Metaworld Dataset - HDF5\n",
    "# \n",
    "# Actions - Difference (next - current)\n",
    "\n",
    "\n",
    "for demo in data.keys():\n",
    "    demo_data = data[demo]\n",
    "    timestamps = demo_data['timestamps']\n",
    "    ee_pos_xyz = demo_data['ee_pos_xyz']\n",
    "    # ee_pos_quat = demo_data['ee_pos_quat']\n",
    "    gripper_pos = demo_data['gripper_pos']\n",
    "    print(f\"Demo: {demo}, Gripper Pos: {gripper_pos.shape}\")\n",
    "    cam0 = demo_data['cam0']\n",
    "    # cam1 = demo_data['cam1']\n",
    "\n",
    "    # ee_pos = np.concatenate((ee_pos_xyz, ee_pos_quat), axis=1)\n",
    "    # actions = ee_pos[1:] - ee_pos[:-1]\n",
    "    actions = ee_pos_xyz[1:] - ee_pos_xyz[:-1]\n",
    "\n",
    "    # actions concatenate gripper_pos\n",
    "    actions = np.concatenate((actions, gripper_pos[:-1]), axis=1)\n",
    "    data[demo][\"actions\"] = actions\n",
    "\n",
    "    print(f\"Actions Added: Dimension: {data[demo]['actions'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5cd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the actions\n",
    "# # Concatenate gripper_pos\n",
    "# first_pos = data['demo_0']['ee_pos_xyz'][0]\n",
    "# # gripper_pos = d[0]\n",
    "# first_pos = np.concatenate((first_pos, [0]), axis=0)\n",
    "\n",
    "# print(f\"first_pos: {first_pos}\")\n",
    "\n",
    "# # Add actions\n",
    "# traj = []\n",
    "# check_actions = data[\"demo_0\"][\"actions\"]\n",
    "# for i in check_actions:\n",
    "#     first_pos[:3]+=i[:3]\n",
    "#     first_pos[3:] = i[3:]\n",
    "#     print(first_pos)\n",
    "# # print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969b3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize function for generating sketch images\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "\n",
    "# def find_green_mark(image):\n",
    "#     hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#     lower_green = np.array([50, 100, 100])\n",
    "#     upper_green = np.array([70, 255, 255])\n",
    "#     mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "#     contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     if contours:\n",
    "#         largest_contour = max(contours, key=cv2.contourArea)\n",
    "#         M = cv2.moments(largest_contour)\n",
    "#         if M[\"m00\"] != 0:\n",
    "#             cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "#             cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "#             return cx, cy\n",
    "#     return None\n",
    "\n",
    "# def process_images(images):\n",
    "#     trajectories = []\n",
    "#     for image_data in images:\n",
    "#         image = np.array(image_data)\n",
    "#         if image is not None:\n",
    "#             position = find_green_mark(image)\n",
    "#             if position:\n",
    "#                 trajectories.append(position)\n",
    "#     return trajectories\n",
    "\n",
    "# def create_blank_canvas(image):\n",
    "#     height, width, channels = image.shape\n",
    "#     return np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "# def draw_trajectory_on_image(trajectories, image):\n",
    "#     for i, (x, y) in enumerate(trajectories):\n",
    "#         color = (255, 255, 0)\n",
    "#         cv2.circle(image, (x, y), 2, color, -1)    \n",
    "#         if i == 0:\n",
    "#             color = (0, 255, 0)\n",
    "#             cv2.circle(image, (x, y), 5, color, -1)\n",
    "#         elif i == len(trajectories) - 1:\n",
    "#             color = (255, 0, 0)\n",
    "#             cv2.circle(image, (x, y), 5, color, -1)\n",
    "#     return image\n",
    "\n",
    "\n",
    "\n",
    "# def generate_sketch_image(camera_images):\n",
    "#     trajectories = process_images(camera_images)\n",
    "#     first_image = camera_images[0]\n",
    "#     final_image = draw_trajectory_on_image(trajectories, first_image)\n",
    "#     blank_canvas = create_blank_canvas(first_image)\n",
    "#     sketch_image = draw_trajectory_on_image(trajectories, blank_canvas)\n",
    "\n",
    "#     return final_image, sketch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "598f7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add sketch images\n",
    "# for demo in data.keys():\n",
    "#     # cam1_images = []\n",
    "#     # for image in data[demo][\"cam1\"]:\n",
    "#     #     cam1_images.append(np.transpose(image, (1, 2, 0)))\n",
    "#     #     print(cam1_images[0].shape)\n",
    "#     # cam2_images = []\n",
    "#     # for image in data[demo][\"cam2\"]:\n",
    "#     #     cam2_images.append(np.transpose(image, (1, 2, 0)))\n",
    "\n",
    "#     cam1_images = data[demo][\"cam1_orig\"][:]\n",
    "#     cam2_images = data[demo][\"cam2_orig\"][:]\n",
    "#     rgb_sketch1, blank_sketch1 = generate_sketch_image(cam1_images)\n",
    "#     rgb_sketch2, blank_sketch2 = generate_sketch_image(cam2_images)\n",
    "\n",
    "#     data[demo][\"cam1_blank_sketch\"] = np.transpose(blank_sketch1, (2, 0, 1))\n",
    "#     data[demo][\"cam1_rgb_sketch\"] = np.transpose(rgb_sketch1, (2, 0, 1))\n",
    "\n",
    "#     data[demo][\"cam2_blank_sketch\"] = np.transpose(blank_sketch2, (2, 0, 1))\n",
    "#     data[demo][\"cam2_rgb_sketch\"] = np.transpose(rgb_sketch2, (2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfa62260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Dataset - hdf5 Generated.. Wohoooo!\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build HDF5 - Metaworld Dataset\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data_light_50_half.hdf5', 'w') as f:\n",
    "\n",
    "\n",
    "    data_group = f.create_group(\"data\")\n",
    "    \n",
    "\n",
    "\n",
    "    for demo in data.keys():\n",
    "\n",
    "        demo_group = data_group.create_group(demo)\n",
    "\n",
    "        # Create action\n",
    "        demo_group.create_dataset(\"actions\", data=data[demo][\"actions\"])\n",
    "\n",
    "\n",
    "        # Create dones\n",
    "        dones = np.zeros(len(data[demo][\"actions\"]))\n",
    "        dones[-1] = 1\n",
    "        demo_group.create_dataset(\"dones\", data=dones)\n",
    "\n",
    "\n",
    "        # Create obs group\n",
    "        obs_group = demo_group.create_group(\"obs\")\n",
    "        # Create eye_in_hand_image\n",
    "        obs_group.create_dataset(\"eye_in_hand_image\", data=data[demo][\"cam0\"][:-1])\n",
    "        # obs_group.create_dataset(\"corner_image\", data=data[demo][\"cam1\"][:-1])\n",
    "        # obs_group.create_dataset(\"corner2_image\", data=data[demo][\"cam2\"][:-1])\n",
    "\n",
    "        # obs_group.create_dataset(\"corner_rgb_sketch\", data=data[demo][\"cam1_rgb_sketch\"])\n",
    "        # obs_group.create_dataset(\"corner_blank_sketch\", data=data[demo][\"cam1_blank_sketch\"])\n",
    "\n",
    "\n",
    "        # obs_group.create_dataset(\"corner2_rgb_sketch\", data=data[demo][\"cam2_rgb_sketch\"])\n",
    "        # obs_group.create_dataset(\"corner2_blank_sketch\", data=data[demo][\"cam2_blank_sketch\"])\n",
    "        # Create prop\n",
    "        prop = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "        obs_group.create_dataset(\"prop\", data=prop[:-1])\n",
    "\n",
    "\n",
    "        # Create rewards\n",
    "        rewards = np.zeros(len(data[demo][\"actions\"]))\n",
    "        rewards[-1] = 1\n",
    "        demo_group.create_dataset(\"rewards\", data=rewards)\n",
    "\n",
    "\n",
    "        # Create states\n",
    "        demo_group.create_dataset(\"states\", data=prop[:-1])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"Dataset - hdf5 Generated.. Wohoooo!\")\n",
    "print(\"----------------------------\")\n",
    "# print file name from above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa7b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Dataset for cluter visualization - hdf5 Generated.. Wohoooo!\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build HDF5 - Metaworld Dataset\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data_rightleft_v2.hdf5', 'w') as f:\n",
    "    untill_n = 10\n",
    "\n",
    "    data_group = f.create_group(\"data\")\n",
    "    \n",
    "    listiwant = ['demo_0', 'demo_4', 'demo_5', 'demo_8', 'demo_9', 'demo_12']\n",
    "\n",
    "    for demo in listiwant:\n",
    "\n",
    "        demo_group = data_group.create_group(demo)\n",
    "\n",
    "        # Create action\n",
    "        demo_group.create_dataset(\"actions\", data=data[demo][\"actions\"][:untill_n])\n",
    "\n",
    "\n",
    "        # Create dones\n",
    "        dones = np.zeros(len(data[demo][\"actions\"]))\n",
    "        dones[untill_n-1] = 1\n",
    "        demo_group.create_dataset(\"dones\", data=dones[:untill_n])\n",
    "\n",
    "\n",
    "        # Create obs group\n",
    "        obs_group = demo_group.create_group(\"obs\")\n",
    "        # Create eye_in_hand_image\n",
    "        obs_group.create_dataset(\"eye_in_hand_image\", data=data[demo][\"cam0\"][:-1][:untill_n])\n",
    "        # obs_group.create_dataset(\"corner_image\", data=data[demo][\"cam1\"][:-1])\n",
    "        # obs_group.create_dataset(\"corner2_image\", data=data[demo][\"cam2\"][:-1])\n",
    "\n",
    "        # obs_group.create_dataset(\"corner_rgb_sketch\", data=data[demo][\"cam1_rgb_sketch\"])\n",
    "        # obs_group.create_dataset(\"corner_blank_sketch\", data=data[demo][\"cam1_blank_sketch\"])\n",
    "\n",
    "\n",
    "        # obs_group.create_dataset(\"corner2_rgb_sketch\", data=data[demo][\"cam2_rgb_sketch\"])\n",
    "        # obs_group.create_dataset(\"corner2_blank_sketch\", data=data[demo][\"cam2_blank_sketch\"])\n",
    "        # Create prop\n",
    "        prop = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "        obs_group.create_dataset(\"prop\", data=prop[:-1][:untill_n])\n",
    "\n",
    "\n",
    "        # Create rewards\n",
    "        rewards = np.zeros(len(data[demo][\"actions\"]))\n",
    "        rewards[untill_n-1] = 1\n",
    "        demo_group.create_dataset(\"rewards\", data=rewards[:untill_n])\n",
    "\n",
    "\n",
    "        # Create states\n",
    "        demo_group.create_dataset(\"states\", data=prop[:-1][:untill_n])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"Dataset for cluter visualization - hdf5 Generated.. Wohoooo!\")\n",
    "print(\"----------------------------\")\n",
    "# print file name from above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b63263bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing demo_0_no_aug...\n",
      "Processing demo_0_color_jitter...\n",
      "Processing demo_0_crop_resize...\n",
      "Processing demo_0_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_1_no_aug...\n",
      "Processing demo_1_color_jitter...\n",
      "Processing demo_1_crop_resize...\n",
      "Processing demo_1_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_10_no_aug...\n",
      "Processing demo_10_color_jitter...\n",
      "Processing demo_10_crop_resize...\n",
      "Processing demo_10_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_11_no_aug...\n",
      "Processing demo_11_color_jitter...\n",
      "Processing demo_11_crop_resize...\n",
      "Processing demo_11_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_12_no_aug...\n",
      "Processing demo_12_color_jitter...\n",
      "Processing demo_12_crop_resize...\n",
      "Processing demo_12_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_13_no_aug...\n",
      "Processing demo_13_color_jitter...\n",
      "Processing demo_13_crop_resize...\n",
      "Processing demo_13_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_14_no_aug...\n",
      "Processing demo_14_color_jitter...\n",
      "Processing demo_14_crop_resize...\n",
      "Processing demo_14_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_15_no_aug...\n",
      "Processing demo_15_color_jitter...\n",
      "Processing demo_15_crop_resize...\n",
      "Processing demo_15_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_16_no_aug...\n",
      "Processing demo_16_color_jitter...\n",
      "Processing demo_16_crop_resize...\n",
      "Processing demo_16_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_17_no_aug...\n",
      "Processing demo_17_color_jitter...\n",
      "Processing demo_17_crop_resize...\n",
      "Processing demo_17_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_18_no_aug...\n",
      "Processing demo_18_color_jitter...\n",
      "Processing demo_18_crop_resize...\n",
      "Processing demo_18_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_19_no_aug...\n",
      "Processing demo_19_color_jitter...\n",
      "Processing demo_19_crop_resize...\n",
      "Processing demo_19_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_2_no_aug...\n",
      "Processing demo_2_color_jitter...\n",
      "Processing demo_2_crop_resize...\n",
      "Processing demo_2_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_20_no_aug...\n",
      "Processing demo_20_color_jitter...\n",
      "Processing demo_20_crop_resize...\n",
      "Processing demo_20_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_21_no_aug...\n",
      "Processing demo_21_color_jitter...\n",
      "Processing demo_21_crop_resize...\n",
      "Processing demo_21_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_22_no_aug...\n",
      "Processing demo_22_color_jitter...\n",
      "Processing demo_22_crop_resize...\n",
      "Processing demo_22_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_23_no_aug...\n",
      "Processing demo_23_color_jitter...\n",
      "Processing demo_23_crop_resize...\n",
      "Processing demo_23_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_24_no_aug...\n",
      "Processing demo_24_color_jitter...\n",
      "Processing demo_24_crop_resize...\n",
      "Processing demo_24_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_25_no_aug...\n",
      "Processing demo_25_color_jitter...\n",
      "Processing demo_25_crop_resize...\n",
      "Processing demo_25_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_26_no_aug...\n",
      "Processing demo_26_color_jitter...\n",
      "Processing demo_26_crop_resize...\n",
      "Processing demo_26_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_27_no_aug...\n",
      "Processing demo_27_color_jitter...\n",
      "Processing demo_27_crop_resize...\n",
      "Processing demo_27_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_28_no_aug...\n",
      "Processing demo_28_color_jitter...\n",
      "Processing demo_28_crop_resize...\n",
      "Processing demo_28_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_29_no_aug...\n",
      "Processing demo_29_color_jitter...\n",
      "Processing demo_29_crop_resize...\n",
      "Processing demo_29_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_3_no_aug...\n",
      "Processing demo_3_color_jitter...\n",
      "Processing demo_3_crop_resize...\n",
      "Processing demo_3_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_30_no_aug...\n",
      "Processing demo_30_color_jitter...\n",
      "Processing demo_30_crop_resize...\n",
      "Processing demo_30_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_31_no_aug...\n",
      "Processing demo_31_color_jitter...\n",
      "Processing demo_31_crop_resize...\n",
      "Processing demo_31_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_32_no_aug...\n",
      "Processing demo_32_color_jitter...\n",
      "Processing demo_32_crop_resize...\n",
      "Processing demo_32_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_33_no_aug...\n",
      "Processing demo_33_color_jitter...\n",
      "Processing demo_33_crop_resize...\n",
      "Processing demo_33_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_34_no_aug...\n",
      "Processing demo_34_color_jitter...\n",
      "Processing demo_34_crop_resize...\n",
      "Processing demo_34_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_35_no_aug...\n",
      "Processing demo_35_color_jitter...\n",
      "Processing demo_35_crop_resize...\n",
      "Processing demo_35_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_36_no_aug...\n",
      "Processing demo_36_color_jitter...\n",
      "Processing demo_36_crop_resize...\n",
      "Processing demo_36_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_37_no_aug...\n",
      "Processing demo_37_color_jitter...\n",
      "Processing demo_37_crop_resize...\n",
      "Processing demo_37_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_38_no_aug...\n",
      "Processing demo_38_color_jitter...\n",
      "Processing demo_38_crop_resize...\n",
      "Processing demo_38_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_39_no_aug...\n",
      "Processing demo_39_color_jitter...\n",
      "Processing demo_39_crop_resize...\n",
      "Processing demo_39_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_4_no_aug...\n",
      "Processing demo_4_color_jitter...\n",
      "Processing demo_4_crop_resize...\n",
      "Processing demo_4_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_40_no_aug...\n",
      "Processing demo_40_color_jitter...\n",
      "Processing demo_40_crop_resize...\n",
      "Processing demo_40_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_41_no_aug...\n",
      "Processing demo_41_color_jitter...\n",
      "Processing demo_41_crop_resize...\n",
      "Processing demo_41_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_42_no_aug...\n",
      "Processing demo_42_color_jitter...\n",
      "Processing demo_42_crop_resize...\n",
      "Processing demo_42_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_43_no_aug...\n",
      "Processing demo_43_color_jitter...\n",
      "Processing demo_43_crop_resize...\n",
      "Processing demo_43_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_44_no_aug...\n",
      "Processing demo_44_color_jitter...\n",
      "Processing demo_44_crop_resize...\n",
      "Processing demo_44_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_45_no_aug...\n",
      "Processing demo_45_color_jitter...\n",
      "Processing demo_45_crop_resize...\n",
      "Processing demo_45_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_46_no_aug...\n",
      "Processing demo_46_color_jitter...\n",
      "Processing demo_46_crop_resize...\n",
      "Processing demo_46_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_47_no_aug...\n",
      "Processing demo_47_color_jitter...\n",
      "Processing demo_47_crop_resize...\n",
      "Processing demo_47_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_48_no_aug...\n",
      "Processing demo_48_color_jitter...\n",
      "Processing demo_48_crop_resize...\n",
      "Processing demo_48_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_49_no_aug...\n",
      "Processing demo_49_color_jitter...\n",
      "Processing demo_49_crop_resize...\n",
      "Processing demo_49_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_5_no_aug...\n",
      "Processing demo_5_color_jitter...\n",
      "Processing demo_5_crop_resize...\n",
      "Processing demo_5_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_6_no_aug...\n",
      "Processing demo_6_color_jitter...\n",
      "Processing demo_6_crop_resize...\n",
      "Processing demo_6_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_7_no_aug...\n",
      "Processing demo_7_color_jitter...\n",
      "Processing demo_7_crop_resize...\n",
      "Processing demo_7_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_8_no_aug...\n",
      "Processing demo_8_color_jitter...\n",
      "Processing demo_8_crop_resize...\n",
      "Processing demo_8_color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_9_no_aug...\n",
      "Processing demo_9_color_jitter...\n",
      "Processing demo_9_crop_resize...\n",
      "Processing demo_9_color_and_crop...\n",
      "----------------------------\n",
      "----------------------------\n",
      "Dataset - HDF5 with Specific Augmented Images Generated for Each Combination.. Wohoooo!\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define augmentation transformations\n",
    "def color_jitter(image):\n",
    "    transform = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "    return transform(image)\n",
    "\n",
    "def random_crop_resize(image):\n",
    "    transform = transforms.RandomResizedCrop(size=(96, 96), scale=(0.8, 1.0), ratio=(0.9, 1.1))\n",
    "    return transform(image)\n",
    "\n",
    "# Apply specific combinations of augmentations\n",
    "def apply_combinations(image):\n",
    "    return {\n",
    "        'no_aug': image,\n",
    "        'color_jitter': color_jitter(image),\n",
    "        'crop_resize': random_crop_resize(image),\n",
    "        'color_and_crop': color_jitter(random_crop_resize(image))\n",
    "    }\n",
    "\n",
    "# Paths to original and new HDF5 files\n",
    "original_hdf5_path = '../data_50_half_mm.hdf5'\n",
    "augmented_hdf5_path = 'data_50_half_mm_augmented_combinations.hdf5'\n",
    "\n",
    "# Processing and saving augmented images\n",
    "with h5py.File(original_hdf5_path, 'r') as original_file, h5py.File(augmented_hdf5_path, 'w') as augmented_file:\n",
    "    data_group = original_file['data']\n",
    "    augmented_data_group = augmented_file.create_group(\"data\")\n",
    "    \n",
    "    for demo in data_group.keys():\n",
    "        demo_group = data_group[demo]\n",
    "        actions = demo_group['actions'][:]\n",
    "        dones = demo_group['dones'][:]\n",
    "        prop = demo_group['obs/prop'][:]\n",
    "        rewards = demo_group['rewards'][:]\n",
    "        states = demo_group['states'][:]\n",
    "        eye_in_hand_images = demo_group['obs/eye_in_hand_image'][:]\n",
    "\n",
    "        # Reorder dimensions for each image from (C, H, W) to (H, W, C)\n",
    "        eye_in_hand_images = eye_in_hand_images.transpose(0, 2, 3, 1)\n",
    "\n",
    "        # For each image augmentation combination, create a new augmented group\n",
    "        for combo_name in ['no_aug', 'color_jitter', 'crop_resize', 'color_and_crop']:\n",
    "            new_demo_name = f\"{demo}_{combo_name}\"\n",
    "            new_demo_group = augmented_data_group.create_group(new_demo_name)\n",
    "            \n",
    "            # Copy data that does not need augmentation\n",
    "            new_demo_group.create_dataset(\"actions\", data=actions)\n",
    "            new_demo_group.create_dataset(\"dones\", data=dones)\n",
    "            obs_group = new_demo_group.create_group(\"obs\")\n",
    "            \n",
    "            print(f\"Processing {new_demo_name}...\")\n",
    "            # Apply augmentation combination to all images in the demo\n",
    "            augmented_images = []\n",
    "            for img in eye_in_hand_images:\n",
    "                pil_image = transforms.ToPILImage()(img)\n",
    "                aug_image = apply_combinations(pil_image)[combo_name]  # Choose augmentation function\n",
    "                tensor_image = transforms.ToTensor()(aug_image).numpy()  # Convert back to tensor and numpy\n",
    "                augmented_images.append(tensor_image)\n",
    "\n",
    "            # Stack augmented images and reorder to (N, C, H, W)\n",
    "            # augmented_images = np.stack(augmented_images).transpose(0, 2, 3, 1)\n",
    "            augmented_images = np.stack(augmented_images)       # Transpose not needed already transposed\n",
    "\n",
    "            # Save augmented images in the new demo group\n",
    "            obs_group.create_dataset(\"eye_in_hand_image\", data=augmented_images)\n",
    "            obs_group.create_dataset(\"prop\", data=prop)\n",
    "            new_demo_group.create_dataset(\"rewards\", data=rewards)\n",
    "            new_demo_group.create_dataset(\"states\", data=states)\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"Dataset - HDF5 with Specific Augmented Images Generated for Each Combination.. Wohoooo!\")\n",
    "print(\"----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6a6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing demo_0 with augmentation no_aug...\n",
      "Processing demo_1 with augmentation color_jitter...\n",
      "Processing demo_2 with augmentation crop_resize...\n",
      "Processing demo_3 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_4 with augmentation no_aug...\n",
      "Processing demo_5 with augmentation color_jitter...\n",
      "Processing demo_6 with augmentation crop_resize...\n",
      "Processing demo_7 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_8 with augmentation no_aug...\n",
      "Processing demo_9 with augmentation color_jitter...\n",
      "Processing demo_10 with augmentation crop_resize...\n",
      "Processing demo_11 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_12 with augmentation no_aug...\n",
      "Processing demo_13 with augmentation color_jitter...\n",
      "Processing demo_14 with augmentation crop_resize...\n",
      "Processing demo_15 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_16 with augmentation no_aug...\n",
      "Processing demo_17 with augmentation color_jitter...\n",
      "Processing demo_18 with augmentation crop_resize...\n",
      "Processing demo_19 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_20 with augmentation no_aug...\n",
      "Processing demo_21 with augmentation color_jitter...\n",
      "Processing demo_22 with augmentation crop_resize...\n",
      "Processing demo_23 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_24 with augmentation no_aug...\n",
      "Processing demo_25 with augmentation color_jitter...\n",
      "Processing demo_26 with augmentation crop_resize...\n",
      "Processing demo_27 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_28 with augmentation no_aug...\n",
      "Processing demo_29 with augmentation color_jitter...\n",
      "Processing demo_30 with augmentation crop_resize...\n",
      "Processing demo_31 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_32 with augmentation no_aug...\n",
      "Processing demo_33 with augmentation color_jitter...\n",
      "Processing demo_34 with augmentation crop_resize...\n",
      "Processing demo_35 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_36 with augmentation no_aug...\n",
      "Processing demo_37 with augmentation color_jitter...\n",
      "Processing demo_38 with augmentation crop_resize...\n",
      "Processing demo_39 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_40 with augmentation no_aug...\n",
      "Processing demo_41 with augmentation color_jitter...\n",
      "Processing demo_42 with augmentation crop_resize...\n",
      "Processing demo_43 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_44 with augmentation no_aug...\n",
      "Processing demo_45 with augmentation color_jitter...\n",
      "Processing demo_46 with augmentation crop_resize...\n",
      "Processing demo_47 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_48 with augmentation no_aug...\n",
      "Processing demo_49 with augmentation color_jitter...\n",
      "Processing demo_50 with augmentation crop_resize...\n",
      "Processing demo_51 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_52 with augmentation no_aug...\n",
      "Processing demo_53 with augmentation color_jitter...\n",
      "Processing demo_54 with augmentation crop_resize...\n",
      "Processing demo_55 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_56 with augmentation no_aug...\n",
      "Processing demo_57 with augmentation color_jitter...\n",
      "Processing demo_58 with augmentation crop_resize...\n",
      "Processing demo_59 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_60 with augmentation no_aug...\n",
      "Processing demo_61 with augmentation color_jitter...\n",
      "Processing demo_62 with augmentation crop_resize...\n",
      "Processing demo_63 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_64 with augmentation no_aug...\n",
      "Processing demo_65 with augmentation color_jitter...\n",
      "Processing demo_66 with augmentation crop_resize...\n",
      "Processing demo_67 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_68 with augmentation no_aug...\n",
      "Processing demo_69 with augmentation color_jitter...\n",
      "Processing demo_70 with augmentation crop_resize...\n",
      "Processing demo_71 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_72 with augmentation no_aug...\n",
      "Processing demo_73 with augmentation color_jitter...\n",
      "Processing demo_74 with augmentation crop_resize...\n",
      "Processing demo_75 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_76 with augmentation no_aug...\n",
      "Processing demo_77 with augmentation color_jitter...\n",
      "Processing demo_78 with augmentation crop_resize...\n",
      "Processing demo_79 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_80 with augmentation no_aug...\n",
      "Processing demo_81 with augmentation color_jitter...\n",
      "Processing demo_82 with augmentation crop_resize...\n",
      "Processing demo_83 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_84 with augmentation no_aug...\n",
      "Processing demo_85 with augmentation color_jitter...\n",
      "Processing demo_86 with augmentation crop_resize...\n",
      "Processing demo_87 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_88 with augmentation no_aug...\n",
      "Processing demo_89 with augmentation color_jitter...\n",
      "Processing demo_90 with augmentation crop_resize...\n",
      "Processing demo_91 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_92 with augmentation no_aug...\n",
      "Processing demo_93 with augmentation color_jitter...\n",
      "Processing demo_94 with augmentation crop_resize...\n",
      "Processing demo_95 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_96 with augmentation no_aug...\n",
      "Processing demo_97 with augmentation color_jitter...\n",
      "Processing demo_98 with augmentation crop_resize...\n",
      "Processing demo_99 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_100 with augmentation no_aug...\n",
      "Processing demo_101 with augmentation color_jitter...\n",
      "Processing demo_102 with augmentation crop_resize...\n",
      "Processing demo_103 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_104 with augmentation no_aug...\n",
      "Processing demo_105 with augmentation color_jitter...\n",
      "Processing demo_106 with augmentation crop_resize...\n",
      "Processing demo_107 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_108 with augmentation no_aug...\n",
      "Processing demo_109 with augmentation color_jitter...\n",
      "Processing demo_110 with augmentation crop_resize...\n",
      "Processing demo_111 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_112 with augmentation no_aug...\n",
      "Processing demo_113 with augmentation color_jitter...\n",
      "Processing demo_114 with augmentation crop_resize...\n",
      "Processing demo_115 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_116 with augmentation no_aug...\n",
      "Processing demo_117 with augmentation color_jitter...\n",
      "Processing demo_118 with augmentation crop_resize...\n",
      "Processing demo_119 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_120 with augmentation no_aug...\n",
      "Processing demo_121 with augmentation color_jitter...\n",
      "Processing demo_122 with augmentation crop_resize...\n",
      "Processing demo_123 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_124 with augmentation no_aug...\n",
      "Processing demo_125 with augmentation color_jitter...\n",
      "Processing demo_126 with augmentation crop_resize...\n",
      "Processing demo_127 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_128 with augmentation no_aug...\n",
      "Processing demo_129 with augmentation color_jitter...\n",
      "Processing demo_130 with augmentation crop_resize...\n",
      "Processing demo_131 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_132 with augmentation no_aug...\n",
      "Processing demo_133 with augmentation color_jitter...\n",
      "Processing demo_134 with augmentation crop_resize...\n",
      "Processing demo_135 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_136 with augmentation no_aug...\n",
      "Processing demo_137 with augmentation color_jitter...\n",
      "Processing demo_138 with augmentation crop_resize...\n",
      "Processing demo_139 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_140 with augmentation no_aug...\n",
      "Processing demo_141 with augmentation color_jitter...\n",
      "Processing demo_142 with augmentation crop_resize...\n",
      "Processing demo_143 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_144 with augmentation no_aug...\n",
      "Processing demo_145 with augmentation color_jitter...\n",
      "Processing demo_146 with augmentation crop_resize...\n",
      "Processing demo_147 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_148 with augmentation no_aug...\n",
      "Processing demo_149 with augmentation color_jitter...\n",
      "Processing demo_150 with augmentation crop_resize...\n",
      "Processing demo_151 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_152 with augmentation no_aug...\n",
      "Processing demo_153 with augmentation color_jitter...\n",
      "Processing demo_154 with augmentation crop_resize...\n",
      "Processing demo_155 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_156 with augmentation no_aug...\n",
      "Processing demo_157 with augmentation color_jitter...\n",
      "Processing demo_158 with augmentation crop_resize...\n",
      "Processing demo_159 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_160 with augmentation no_aug...\n",
      "Processing demo_161 with augmentation color_jitter...\n",
      "Processing demo_162 with augmentation crop_resize...\n",
      "Processing demo_163 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_164 with augmentation no_aug...\n",
      "Processing demo_165 with augmentation color_jitter...\n",
      "Processing demo_166 with augmentation crop_resize...\n",
      "Processing demo_167 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_168 with augmentation no_aug...\n",
      "Processing demo_169 with augmentation color_jitter...\n",
      "Processing demo_170 with augmentation crop_resize...\n",
      "Processing demo_171 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_172 with augmentation no_aug...\n",
      "Processing demo_173 with augmentation color_jitter...\n",
      "Processing demo_174 with augmentation crop_resize...\n",
      "Processing demo_175 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_176 with augmentation no_aug...\n",
      "Processing demo_177 with augmentation color_jitter...\n",
      "Processing demo_178 with augmentation crop_resize...\n",
      "Processing demo_179 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_180 with augmentation no_aug...\n",
      "Processing demo_181 with augmentation color_jitter...\n",
      "Processing demo_182 with augmentation crop_resize...\n",
      "Processing demo_183 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_184 with augmentation no_aug...\n",
      "Processing demo_185 with augmentation color_jitter...\n",
      "Processing demo_186 with augmentation crop_resize...\n",
      "Processing demo_187 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_188 with augmentation no_aug...\n",
      "Processing demo_189 with augmentation color_jitter...\n",
      "Processing demo_190 with augmentation crop_resize...\n",
      "Processing demo_191 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_192 with augmentation no_aug...\n",
      "Processing demo_193 with augmentation color_jitter...\n",
      "Processing demo_194 with augmentation crop_resize...\n",
      "Processing demo_195 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "Processing demo_196 with augmentation no_aug...\n",
      "Processing demo_197 with augmentation color_jitter...\n",
      "Processing demo_198 with augmentation crop_resize...\n",
      "Processing demo_199 with augmentation color_and_crop...\n",
      "----------------------------\n",
      "----------------------------\n",
      "Dataset - HDF5 with Specific Augmented Images Generated for Each Combination.. Wohoooo!\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# auguementation with demo counter\n",
    "\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define augmentation transformations\n",
    "def color_jitter(image):\n",
    "    transform = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "    return transform(image)\n",
    "\n",
    "def random_crop_resize(image):\n",
    "    transform = transforms.RandomResizedCrop(size=(96, 96), scale=(0.8, 1.0), ratio=(0.9, 1.1))\n",
    "    return transform(image)\n",
    "\n",
    "# Apply specific combinations of augmentations\n",
    "def apply_combinations(image):\n",
    "    return {\n",
    "        'no_aug': image,\n",
    "        'color_jitter': color_jitter(image),\n",
    "        'crop_resize': random_crop_resize(image),\n",
    "        'color_and_crop': color_jitter(random_crop_resize(image))\n",
    "    }\n",
    "\n",
    "# Paths to original and new HDF5 files\n",
    "original_hdf5_path = '../data_50_half.hdf5'\n",
    "augmented_hdf5_path = 'data_50_half_augmented.hdf5'\n",
    "\n",
    "# Initialize counter for unique demo names\n",
    "demo_counter = 0\n",
    "\n",
    "# Processing and saving augmented images\n",
    "with h5py.File(original_hdf5_path, 'r') as original_file, h5py.File(augmented_hdf5_path, 'w') as augmented_file:\n",
    "    data_group = original_file['data']\n",
    "    augmented_data_group = augmented_file.create_group(\"data\")\n",
    "    \n",
    "    for demo in data_group.keys():\n",
    "        demo_group = data_group[demo]\n",
    "        actions = demo_group['actions'][:]\n",
    "        dones = demo_group['dones'][:]\n",
    "        prop = demo_group['obs/prop'][:]\n",
    "        rewards = demo_group['rewards'][:]\n",
    "        states = demo_group['states'][:]\n",
    "        eye_in_hand_images = demo_group['obs/eye_in_hand_image'][:]\n",
    "\n",
    "        # Reorder dimensions for each image from (C, H, W) to (H, W, C)\n",
    "        eye_in_hand_images = eye_in_hand_images.transpose(0, 2, 3, 1)\n",
    "\n",
    "        # For each image augmentation combination, create a new augmented group\n",
    "        for combo_name in ['no_aug', 'color_jitter', 'crop_resize', 'color_and_crop']:\n",
    "            new_demo_name = f\"demo_{demo_counter}\"  # Use demo_counter for unique demo name\n",
    "            new_demo_group = augmented_data_group.create_group(new_demo_name)\n",
    "            demo_counter += 1  # Increment the counter after creating each new demo\n",
    "\n",
    "            # Copy data that does not need augmentation\n",
    "            new_demo_group.create_dataset(\"actions\", data=actions)\n",
    "            new_demo_group.create_dataset(\"dones\", data=dones)\n",
    "            obs_group = new_demo_group.create_group(\"obs\")\n",
    "            \n",
    "            print(f\"Processing {new_demo_name} with augmentation {combo_name}...\")\n",
    "            \n",
    "            # Apply augmentation combination to all images in the demo\n",
    "            augmented_images = []\n",
    "            for img in eye_in_hand_images:\n",
    "                pil_image = transforms.ToPILImage()(img)\n",
    "                aug_image = apply_combinations(pil_image)[combo_name]  # Choose augmentation function\n",
    "                tensor_image = transforms.ToTensor()(aug_image).numpy()  # Convert back to tensor and numpy\n",
    "                augmented_images.append(tensor_image)\n",
    "\n",
    "            # Stack augmented images (no need to transpose as already in (N, C, H, W))\n",
    "            augmented_images = np.stack(augmented_images)\n",
    "\n",
    "            # Save augmented images in the new demo group\n",
    "            obs_group.create_dataset(\"eye_in_hand_image\", data=augmented_images)\n",
    "            obs_group.create_dataset(\"prop\", data=prop)\n",
    "            new_demo_group.create_dataset(\"rewards\", data=rewards)\n",
    "            new_demo_group.create_dataset(\"states\", data=states)\n",
    "        \n",
    "        print(\"----------------------------\")\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"Dataset - HDF5 with Specific Augmented Images Generated for Each Combination.. Wohoooo!\")\n",
    "print(\"----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fdd81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add sketch images too\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "\n",
    "# def find_green_mark(image):\n",
    "#     hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#     lower_green = np.array([50, 100, 100])\n",
    "#     upper_green = np.array([70, 255, 255])\n",
    "#     mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "#     contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     if contours:\n",
    "#         largest_contour = max(contours, key=cv2.contourArea)\n",
    "#         M = cv2.moments(largest_contour)\n",
    "#         if M[\"m00\"] != 0:\n",
    "#             cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "#             cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "#             return cx, cy\n",
    "#     return None\n",
    "\n",
    "# def process_images(images):\n",
    "#     trajectories = []\n",
    "#     for image_data in images:\n",
    "#         image = np.array(image_data)\n",
    "#         if image is not None:\n",
    "#             position = find_green_mark(image)\n",
    "#             if position:\n",
    "#                 trajectories.append(position)\n",
    "#     return trajectories\n",
    "\n",
    "# def create_blank_canvas(image):\n",
    "#     height, width, channels = image.shape\n",
    "#     return np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "# def draw_trajectory_on_image(trajectories, image):\n",
    "#     for i, (x, y) in enumerate(trajectories):\n",
    "#         color = (0, 0, 255)\n",
    "#         if i == 0:\n",
    "#             color = (0, 255, 0)\n",
    "#             cv2.circle(image, (x, y), 4, color, -1)\n",
    "#         elif i == len(trajectories) - 1:\n",
    "#             color = (255, 0, 0)\n",
    "#             cv2.circle(image, (x, y), 4, color, -1)\n",
    "#         else:\n",
    "#             cv2.circle(image, (x, y), 2, color, -1)\n",
    "#     return image\n",
    "\n",
    "# def process_hdf5_file(hdf5_path):\n",
    "#     with h5py.File(hdf5_path, 'a') as file:\n",
    "#         print(file['data'].keys())\n",
    "#         for demo in file['data'].keys():\n",
    "#             camera_name = [\"corner2_image\", \"corner_image\", \"eye_in_hand_image\"]\n",
    "#             # for dataset_name, [blank_dataset, rgb_dataset] in datasets.items():\n",
    "#             for cam in camera_name:\n",
    "#                 # images = file[dataset_name]\n",
    "#                 images = file[f\"data/{demo}/obs/{cam}\"][:]\n",
    "#                 trajectories = process_images(images)\n",
    "#                 first_image = np.array(images[0]) if images else None\n",
    "#                 final_image = draw_trajectory_on_image(trajectories, first_image)\n",
    "#                 blank_canvas = create_blank_canvas(first_image)\n",
    "#                 sketch_image = draw_trajectory_on_image(trajectories, blank_canvas)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# hdf5_path = 'data.hdf5'\n",
    "# process_hdf5_file(hdf5_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5024467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets build dataset - Robosuite Dataset\n",
    "# # \n",
    "# # 1. -- Ignored attributes from default dataset --\n",
    "# #       obs - [object, robot0_eef_vel_ang, robot0_eef_vel_lin, robot0_gripper_qvel, \n",
    "# #               robot0_joint_pos_cos, robot0_joint_pos_sin, robot0_joint_vel_cos, \n",
    "# #               robot0_joint_vel_sin, robot0_joint_vel]\n",
    "# # \n",
    "# # 2. Skipped some columns in of states\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# with h5py.File('data/dataset_2024_7_19/data_robosuite.hdf5', 'w') as f:\n",
    "\n",
    "#     data_group_r = f.create_group(\"data\")\n",
    "\n",
    "#     for demo in data.keys():\n",
    "        \n",
    "#         demo_group_r = data_group_r.create_group(demo)\n",
    "\n",
    "#         # Create actions\n",
    "#         demo_group_r.create_dataset(\"actions\", data=data[demo][\"actions\"])\n",
    "\n",
    "#         # Create dones\n",
    "#         dones_r = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         dones_r[-1] = 1\n",
    "#         demo_group_r.create_dataset(\"dones\", data=dones_r)\n",
    "\n",
    "#         # Create obs group\n",
    "#         obs_group_r = demo_group_r.create_group(\"obs\")\n",
    "#         # Create agentview_image\n",
    "#         obs_group_r.create_dataset(\"agentview_image\", data=data[demo][\"images\"][:-1])\n",
    "#         # Create robot0_eef_pos\n",
    "#         obs_group_r.create_dataset(\"robot0_eef_pos\", data=data[demo][\"ee_pos_rpy\"][:, :3][:-1])\n",
    "#         # Create robot0_eef_rpy\n",
    "#         obs_group_r.create_dataset(\"robot0_eef_rpy\", data=data[demo][\"ee_pos_rpy\"][:, 3:][:-1])\n",
    "#         # Create robot0_gripper_qpos\n",
    "#         obs_group_r.create_dataset(\"robot0_gripper_qpos\", data=data[demo][\"gripper_pos\"][:-1])\n",
    "#         # Create robot0_joint_pos\n",
    "#         obs_group_r.create_dataset(\"robot0_joint_pos\", data=data[demo][\"joint_angles\"][:-1])\n",
    "\n",
    "#         # Create rewards\n",
    "#         rewards_r = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         rewards_r[-1] = 1\n",
    "#         demo_group_r.create_dataset(\"rewards\", data=rewards_r)\n",
    "\n",
    "#         # Create states\n",
    "#         # [time, joint_angles, gripper_pos, ee_pos_rpy, first_obj_pose, second_obj_pose]\n",
    "#         states = np.concatenate((data[demo][\"joint_angles\"], data[demo][\"gripper_pos\"], data[demo][\"ee_pos_rpy\"]), axis=1)\n",
    "#         demo_group_r.create_dataset(\"states\", data=states[:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e1b7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset - Metaworld [Bad Data - demo1 and demo6]\n",
    "\n",
    "\n",
    "# # def change_gripper_pos(gripper_pos_array):\n",
    "# #     indices = [46, 49, 47, 47, 49, 43, 41, 50]\n",
    "# #     gripper_pos = np.zeros()\n",
    "\n",
    "\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# with h5py.File('data/dataset_2024_7_25_2/data_processed_new.hdf5', 'w') as f:\n",
    "\n",
    "\n",
    "#     data_group_b = f.create_group(\"data\")\n",
    "\n",
    "#     indices = [46, 49, 47, 47, 49, 43, 41, 50]\n",
    "\n",
    "\n",
    "#     n = 0\n",
    "#     for demo in data.keys():\n",
    "\n",
    "#         if demo == \"demo_5\":\n",
    "#             continue\n",
    "\n",
    "#         demo_group_b = data_group_b.create_group(f\"demo_{n}\")\n",
    "\n",
    "#         # Create action\n",
    "#         demo_group_b.create_dataset(\"actions\", data=data[demo][\"actions\"])\n",
    "\n",
    "#         # Create dones\n",
    "#         dones_b = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         dones_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"dones\", data=dones_b)\n",
    "\n",
    "\n",
    "#         # Create obs group\n",
    "#         obs_group_b = demo_group_b.create_group(\"obs\")\n",
    "#         # Create corner2_image\n",
    "#         obs_group_b.create_dataset(\"eye_in_hand_image\", data=data[demo][\"cam0\"][:-1])\n",
    "#         obs_group_b.create_dataset(\"front_image\", data=data[demo][\"cam1\"][:-1])\n",
    "#         # obs_group_b.create_dataset(\"eye_in_hand_image\", data=data[demo][\"images\"][:-1])\n",
    "#         # Create prop\n",
    "#         # ee_pos = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"ee_pos_quat\"]), axis=1)\n",
    "#         prop_b = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "#         obs_group_b.create_dataset(\"prop\", data=prop_b[:-1])\n",
    "\n",
    "\n",
    "#         # Create rewards\n",
    "#         rewards_b = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         rewards_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"rewards\", data=rewards_b)\n",
    "\n",
    "\n",
    "#         # Create states\n",
    "#         states_b = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "#         demo_group_b.create_dataset(\"states\", data=states_b[:-1])\n",
    "\n",
    "#         n+=1\n",
    "\n",
    "# print(\"----------------------------\")\n",
    "# print(\"Processed Dataset - hdf5 Generated\")\n",
    "# print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2334cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## --- remove some points in the dataset -----------\n",
    "\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# with h5py.File('data/dataset_2024_07_25/data_removed_lift.hdf5', 'w') as f:\n",
    "    \n",
    "#     data_group_b = f.create_group(\"data\")\n",
    "\n",
    "#     indices = [46, 49, 47, 47, 49, 43, 41, 50]\n",
    "\n",
    "\n",
    "#     n = 0\n",
    "#     for demo in data.keys():\n",
    "\n",
    "#         if demo == \"demo_1\":\n",
    "#             continue\n",
    "\n",
    "#         demo_group_b = data_group_b.create_group(f\"demo_{n}\")\n",
    "\n",
    "#         # Create action\n",
    "#         demo_group_b.create_dataset(\"actions\", data=data[demo][\"actions\"][:indices[n]+1])\n",
    "\n",
    "#         # Create dones\n",
    "#         dones_b = np.zeros(len(data[demo][\"actions\"][:indices[n]+1]))\n",
    "#         dones_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"dones\", data=dones_b)\n",
    "\n",
    "\n",
    "#         # Create obs group\n",
    "#         obs_group_b = demo_group_b.create_group(\"obs\")\n",
    "#         # Create corner2_image\n",
    "#         obs_group_b.create_dataset(\"corner2_image\", data=data[demo][\"cam0\"][:indices[n]+1])\n",
    "#         obs_group_b.create_dataset(\"front_image\", data=data[demo][\"cam1\"][:indices[n]+1])\n",
    "#         # obs_group_b.create_dataset(\"corner2_image\", data=data[demo][\"images\"][:-1])\n",
    "#         # Create prop\n",
    "#         # ee_pos = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"ee_pos_quat\"]), axis=1)\n",
    "#         prop_b = np.concatenate((data[demo][\"ee_pos_xyz\"][:indices[n]+1], data[demo][\"gripper_pos\"][:indices[n]+1]), axis=1)\n",
    "#         obs_group_b.create_dataset(\"prop\", data=prop_b)\n",
    "\n",
    "\n",
    "#         # Create rewards\n",
    "#         rewards_b = np.zeros(len(data[demo][\"actions\"][:indices[n]+1]))\n",
    "#         rewards_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"rewards\", data=rewards_b)\n",
    "\n",
    "\n",
    "#         # Create states\n",
    "#         states_b = np.concatenate((data[demo][\"ee_pos_xyz\"][:indices[n]+1], data[demo][\"gripper_pos\"][:indices[n]+1]), axis=1)\n",
    "#         demo_group_b.create_dataset(\"states\", data=states_b)\n",
    "\n",
    "#         n+=1\n",
    "\n",
    "# print(\"----------------------------\")\n",
    "# print(\"Removed Dataset - hdf5 Generated\")\n",
    "# print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50da07a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'data/dataset_2024_07_25/datanew.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/dataset_2024_07_25/datanew.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     data_group_b \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     all_x \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/gelloenv/lib/python3.12/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/gelloenv/lib/python3.12/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'data/dataset_2024_07_25/datanew.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "## Get (min, max) of x, y, z\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File('data/dataset_2024_07_25/datanew.hdf5', 'r') as f:\n",
    "    \n",
    "    data_group_b = f[\"data\"]\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    all_z = []\n",
    "    for demo in data_group_b:\n",
    "        states = data_group_b[demo][\"states\"][:]\n",
    "        x = states[:,0]\n",
    "        y = states[:,1]\n",
    "        z = states[:,2]\n",
    "        # w_ = states[:,3]\n",
    "        # x_ = states[:,4]\n",
    "        # y_ = states[:,5]\n",
    "        # z_ = states[:,6]\n",
    "        all_x.append(x)\n",
    "        all_y.append(y)\n",
    "        all_z.append(z)\n",
    "\n",
    "    all_x = np.concatenate(all_x)\n",
    "    all_y = np.concatenate(all_y)\n",
    "    all_z = np.concatenate(all_z)\n",
    "    \n",
    "    print(f\"min x: {np.round(np.min(all_x), 3)}\")\n",
    "    print(f\"min y: {np.round(np.min(all_y), 3)}\")\n",
    "    print(f\"min z: {np.round(np.min(all_z), 3)}\")\n",
    "    print(f\"max x: {np.round(np.max(all_x), 3)}\")\n",
    "    print(f\"max y: {np.round(np.max(all_y), 3)}\")\n",
    "    print(f\"max z: {np.round(np.max(all_z), 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gelloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
